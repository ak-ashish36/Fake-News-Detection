{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake news Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required library\n",
    "Here I am importing some of the required library, if extra library is required to install It will be install later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting fake and real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv(\"datasets/Fake.csv\")\n",
    "df_true = pd.read_csv(\"datasets/True.csv\")\n",
    "df_true2 =pd.read_csv(\"datasets/News.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting a column called \"label\" for fake and real news dataset to categories fake and true news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake[\"label\"] = 0\n",
    "df_true[\"label\"] = 1\n",
    "df_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.shape, df_true.shape, df_true2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the manual testing dataframe in single dataset and save it in a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the main fake and true dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marge = pd.concat([df_fake, df_true, df_true2], axis =0 )\n",
    "df_marge.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marge.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"title\",  \"subject\" and \"date\" columns is not required for detecting the fake news, so I am going to drop the columns.\n",
    "## Final Dataset is: df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df_marge.drop([\"title\", \"subject\",\"date\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, subset=['text'])\n",
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly shuffling the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)\n",
    "df.drop([\"index\"], axis = 1, inplace = True)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a function to convert the text in lowercase, remove the extra space, special chr., ulr and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(wordopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining dependent and independent variable as x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"]\n",
    "Y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorization = TfidfVectorizer()\n",
    "X = vectorization.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into training set and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression\n",
    "##### Logistic Regression is a simple linear model that is commonly used for binary classification problems, including fake news detection. It models the relationship between the independent variables and the probability of the binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train,y_train)\n",
    "pred_lr=LR.predict(x_test)\n",
    "score = LR.score(x_test, y_test)\n",
    "# score=accuracy_score(y_test,pred_lr)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "print(classification_report(y_test, pred_lr))\n",
    "confusion_matrix(y_test,pred_lr, labels=[0,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Passive Aggressive Classifier\n",
    "##### PAC is an online learning algorithm, which means it can adapt to changes in the distribution of fake news as new data is received. It works by iteratively updating a linear classifier based on the features of the news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "PAC = PassiveAggressiveClassifier(max_iter=1000)\n",
    "PAC.fit(x_train, y_train)\n",
    "pred_pac = PAC.predict(x_test)\n",
    "score = PAC.score(x_test, y_test)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "print(classification_report(y_test, pred_pac))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. k-Nearest Neighbors (KNN)\n",
    "##### The k-Nearest Neighbors (KNN) algorithm can be used for fake news detection by treating each news article as a vector of features and labels, and finding the k nearest neighbors to a new article based on its feature vector. The label of the new article is then predicted based on the labels of its k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "KNN.fit(x_train, y_train)\n",
    "pred_knn = KNN.predict(x_test)\n",
    "score = KNN.score(x_test, y_test)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "print(classification_report(y_test, pred_knn))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes Classifier\n",
    "##### Naive Bayes is a simple but effective model for text classification problems, including fake news detection. It assumes independence between features and can handle large amounts of data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NBC = MultinomialNB()\n",
    "NBC.fit(x_train, y_train)\n",
    "pred_nbc = NBC.predict(x_test)\n",
    "score = NBC.score(x_test, y_test)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "print(classification_report(y_test, pred_nbc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decision Tree Classifier\n",
    "##### Decision Trees are a simple but powerful model for classification problems. They work by recursively splitting the data based on the most significant feature until all of the instances in a leaf node belong to the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DTC = DecisionTreeClassifier()\n",
    "DTC.fit(x_train, y_train)\n",
    "pred_dt = DTC.predict(x_test)\n",
    "score = DTC.score(x_test, y_test)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "print(classification_report(y_test, pred_dt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Random Forest Classifier\n",
    "##### Random Forest is an ensemble model that uses multiple decision trees to make predictions. It is robust to overfitting and can handle large amounts of data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFC = RandomForestClassifier(random_state=0)\n",
    "RFC.fit(x_train, y_train)\n",
    "pred_rfc = RFC.predict(x_test)\n",
    "score = RFC.score(x_test, y_test)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "print(classification_report(y_test, pred_rfc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Gradient Boosting Classifier\n",
    "##### Gradient Boosting is another ensemble model that uses decision trees as weak learners. It can handle complex data distributions and is often used for text classification problems, including fake news detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(random_state=0)\n",
    "GBC.fit(x_train, y_train)\n",
    "pred_gbc = GBC.predict(x_test)\n",
    "score=GBC.score(x_test, y_test)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "print(classification_report(y_test, pred_gbc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. XGBoost\n",
    "##### XGBoost is an optimized version of the gradient boosting algorithm that has been shown to achieve state-of-the-art results on many machine learning tasks, including fake news detection. It uses gradient boosting with decision trees as weak learners to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "XGB = xgb.XGBClassifier()\n",
    "XGB.fit(x_train, y_train)\n",
    "pred_xgb = XGB.predict(x_test)\n",
    "score = XGB.score(x_test, y_test)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "print(classification_report(y_test, pred_xgb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. LightGBM\n",
    "##### LightGBM is another gradient boosting algorithm that uses decision trees as weak learners. It is designed to handle large amounts of data and has been shown to achieve fast training times and good performance on a variety of tasks, including fake news detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "LGB = lgb.LGBMClassifier()\n",
    "LGB.fit(x_train, y_train)\n",
    "pred_lgb = LGB.predict(x_test)\n",
    "score = XGB.score(x_test, y_test)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "print(classification_report(y_test, pred_lgb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.Support Vector Machine (SVM)\n",
    "##### SVM is a powerful model for text classification problems. It works well with high-dimensional data and can handle non-linear relationships between features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVM = SVC(kernel='linear', C=1, random_state=0)\n",
    "SVM.fit(x_train, y_train)\n",
    "pred_svm = SVM.predict(x_test)\n",
    "score = SVM.score(x_test, y_test)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "print(classification_report(y_test, pred_svm))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Multilayer Perceptron (MLP)\n",
    "##### MLP is a type of artificial neural network that can be used for binary or multiclass classification problems, including fake news detection. It consists of multiple hidden layers of artificial neurons that process the input data and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, alpha=1e-4, solver='sgd', verbose=False, tol=1e-4, random_state=1, learning_rate_init=.1)\n",
    "MLP.fit(x_train, y_train)\n",
    "pred_mlp = MLP.predict(x_test)\n",
    "score = MLP.score(x_test, y_test)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n",
    "print(classification_report(y_test, pred_mlp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(vectorization, \"models/vectorizer.pkl\")\n",
    "joblib.dump(LR, \"models/LR.pkl\")\n",
    "joblib.dump(PAC, \"models/PAC.pkl\")\n",
    "joblib.dump(KNN, \"models/KNN.pkl\")\n",
    "joblib.dump(NBC, \"models/NBC.pkl\")\n",
    "joblib.dump(DTC, \"models/DTC.pkl\")\n",
    "joblib.dump(RFC, \"models/RFC.pkl\")\n",
    "joblib.dump(GBC, \"models/GBC.pkl\")\n",
    "joblib.dump(XGB, \"models/XGB.pkl\")\n",
    "joblib.dump(LGB, \"models/LGB.pkl\")\n",
    "joblib.dump(SVM, \"models/SVM.pkl\")\n",
    "joblib.dump(MLP, \"models/MLP.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing With Manual Entry\n",
    "\n",
    "### News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_lable(n):\n",
    "    if n == 0:\n",
    "        return \"Fake News\"\n",
    "    elif n == 1:\n",
    "        return \"Not A Fake News\"\n",
    "    \n",
    "def manual_testing(news):\n",
    "    testing_news = {\"text\":[news]}\n",
    "    new_def_test = pd.DataFrame(testing_news)\n",
    "    new_def_test[\"text\"] = new_def_test[\"text\"].apply(wordopt) \n",
    "    new_x_test = new_def_test[\"text\"]\n",
    "    new_xv_test = vectorization.transform(new_x_test)\n",
    "    pred_LR = LR.predict(new_xv_test)\n",
    "    pred_PAC = PAC.predict(new_xv_test)\n",
    "    pred_KNN = KNN.predict(new_xv_test)\n",
    "    pred_NBC = NBC.predict(new_xv_test)\n",
    "    pred_DTC = DTC.predict(new_xv_test)\n",
    "    pred_RFC = RFC.predict(new_xv_test)\n",
    "    pred_GBC = GBC.predict(new_xv_test)\n",
    "    pred_XGB = XGB.predict(new_xv_test)\n",
    "    pred_LGB = LGB.predict(new_xv_test)\n",
    "    pred_SVM = SVM.predict(new_xv_test)\n",
    "    pred_MLP = MLP.predict(new_xv_test)\n",
    "\n",
    "    return print(\"\\n\\nLR Prediction: {} \\nPAC Prediction: {} \\nKNN Prediction: {} \\nNBC Prediction: {}\\nDT Prediction: {} \\nRFC Prediction: {} \\nGBC Prediction: {} \\nXGB Prediction: {} \\nLGB Prediction: {} \\nSVM Prediction: {} \\nMLP Prediction: {}\"\n",
    "    .format(output_lable(pred_LR[0]),\n",
    "            output_lable(pred_PAC[0]),\n",
    "            output_lable(pred_KNN[0]),\n",
    "            output_lable(pred_NBC[0]),\n",
    "            output_lable(pred_DTC[0]), \n",
    "            output_lable(pred_RFC[0]),\n",
    "            output_lable(pred_GBC[0]), \n",
    "            output_lable(pred_XGB[0]),\n",
    "            output_lable(pred_LGB[0]),\n",
    "            output_lable(pred_SVM[0]), \n",
    "            output_lable(pred_MLP[0]),\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = str(input())\n",
    "manual_testing(news)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8882a89171e2c2f955b05c156ca5c411b8c7a75ce9ce5c79b3f3290aa56be59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
